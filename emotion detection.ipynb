{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6527105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import  DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a630a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = 'Datasets'\n",
    "train_path = 'Datasets/train'\n",
    "test_path ='Datasets/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57598a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations for training data (with augmentation)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1), # Add grayscale transformation\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5],  # Placeholder values - will be updated after recalculating mean/std\n",
    "                         [0.5])\n",
    "])\n",
    "\n",
    "# Transformations for validation data (no augmentation)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1), # Add grayscale transformation\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5],  # Placeholder values - will be updated after recalculating mean/std\n",
    "                         [0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac163c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset dari struktur folder\n",
    "train_set = datasets.ImageFolder(root=train_path, transform=train_transform)\n",
    "test_set  = datasets.ImageFolder(root=test_path,  transform=val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ec34d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train_set â†’ 70% train, 20% val\n",
    "total_test = len(test_set)\n",
    "test_size  = int(0.5 * total_test)  # 50% test\n",
    "val_size    = total_test - test_size  # 50% val\n",
    "\n",
    "test_dataset, val_dataset = random_split(test_set, [test_size, val_size])\n",
    "\n",
    "# train_set tetap dipakai sebagai train_dataset\n",
    "train_dataset = train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac03762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4885c3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 28709\n",
      "Validation samples: 3589\n",
      "Test samples: 3589\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6728417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimalCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AnimalCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool  = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.fc1   = nn.Linear(128 * 16 * 16, 256)\n",
    "        self.fc2   = nn.Linear(256, 7)  # 7 class: emosi\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # 32x112x112 â†’ 32x56x56\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # 64x56x56  â†’ 64x28x28\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # 128x28x28 â†’ 128x14x14\n",
    "        # x = x.view(-1, 128 * 14 * 14)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "12cb0255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# Initialize model and move to device\n",
    "model = AnimalCNN().to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fead48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, epochs=10, device='cpu'):\n",
    "    model.to(device)\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # === TRAINING PHASE ===\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100 * correct / total\n",
    "        print(f\"âœ… Training   - Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "        # === VALIDATION PHASE ===\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        print(f\"ðŸ“Š Validation - Loss: {avg_val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "        # === SAVE BEST MODEL ===\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(\"ðŸ”¥ Best model disimpan: best_model.pth\")\n",
    "\n",
    "    print(\"\\nðŸŽ¯ Training selesai!\")\n",
    "\n",
    "    # === TESTING PHASE (Setelah Semua Epoch) ===\n",
    "    print(\"\\n--- Evaluasi Akhir pada Test Set ---\")\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    test_acc = 100 * test_correct / test_total\n",
    "    print(f\"\\nðŸ§ª Test Final - Loss: {avg_test_loss:.4f}, Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c4a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71b02a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "# Access class names correctly for Subset\n",
    "class_names = val_dataset.dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13daf62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_true, y_pred, target_names=class_names)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec00dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22929b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the dataset with the updated transform\n",
    "train_set_gray = datasets.ImageFolder(root=train_path, transform=train_transform)\n",
    "\n",
    "# Recreate the DataLoader with the updated dataset\n",
    "train_loader_gray = DataLoader(train_set_gray, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "# Calculate mean and std for grayscale images\n",
    "def get_mean_std_gray(loader):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    total_images_count = 0\n",
    "\n",
    "    for images, _ in tqdm(loader):\n",
    "        batch_samples = images.size(0)  # jumlah gambar di batch\n",
    "        images = images.view(batch_samples, -1)  # reshape to (batch, pixels)\n",
    "        mean += images.mean(1).sum(0)\n",
    "        std += images.std(1).sum(0)\n",
    "        total_images_count += batch_samples\n",
    "\n",
    "    mean /= total_images_count\n",
    "    std /= total_images_count\n",
    "    return mean, std\n",
    "\n",
    "mean_gray, std_gray = get_mean_std_gray(train_loader_gray)\n",
    "print(\"Mean (Grayscale):\", mean_gray)\n",
    "print(\"Std (Grayscale):\", std_gray)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
